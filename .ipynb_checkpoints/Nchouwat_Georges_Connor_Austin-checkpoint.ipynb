{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> Part A: Model Code </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sortedcontainers import SortedList\n",
    "from future.utils import iteritems\n",
    " \n",
    "def euclidianDistance(a,b):\n",
    "   \n",
    "    \n",
    "    return  np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Manhattan Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ManhattanDistance(a,b):\n",
    "    return np.absolute(np.sum((a-b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Accuracy and generalization error of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function returns the accuracy and the generalization error.\n",
    "Y is the target from the data set.\n",
    "Accuracy is the ratio of the correct predictions and the total number of predictions.\n",
    "General Error = 1 - accuracy \n",
    "'''\n",
    "\n",
    "def accuracyGeneralizationError(Y, Y_predicted ):\n",
    "    accuracy = np.mean(Y == Y_predicted)\n",
    "    gn = 1 - accuracy    \n",
    "    return accuracy, gn  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. precision, recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n",
    "def precision(Y, Y_predicted):\n",
    "    #truePositive = np.sum(Y == Y_predicted).astype(np.int)\n",
    "    truePositive = 0\n",
    "    falsePositive =0\n",
    "    totalTruePositive = 0\n",
    "    for i, k in zip(Y, Y_predicted):\n",
    "        if i ==1 and k == 1:\n",
    "            truePositive +=1\n",
    "            totalTruePositive +=1\n",
    "        if i == 0 and k == 1:\n",
    "            falsePositive +=1\n",
    "            \n",
    "    if  (truePositive + falsePositive) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return truePositive / (truePositive + falsePositive )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall\n",
    "def recall(Y, Y_predicted):\n",
    "    #truePositive = np.sum(Y == Y_predicted).astype(np.int)\n",
    "    truePositive = 0\n",
    "    falseNegative = 0\n",
    "    for i, k in zip(Y, Y_predicted):\n",
    "        if i == 1 and k == 0:\n",
    "            falseNegative +=1\n",
    "        if i ==1 and k ==1:\n",
    "            truePositive += 1\n",
    "            \n",
    "    if (truePositive + falsePositive) == 0:\n",
    "        return 0\n",
    "    else:           \n",
    "        return truePositive / ( truePositive + falseNegative )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score\n",
    "def F1_score(Y, Y_predicted):\n",
    "    \n",
    "    prec =0\n",
    "    falseNegative= 0\n",
    "    truePositive = 0\n",
    "    falsePositive = 0\n",
    "    recal = 0\n",
    "    \n",
    "    for i, k in zip(Y, Y_predicted):\n",
    "        if i == 1 and k == 0:\n",
    "            falseNegative +=1\n",
    "        if i ==1 and k ==1:\n",
    "            truePositive += 1\n",
    "        if i == 0 and k == 1:\n",
    "            falsePositive +=1\n",
    "     \n",
    "    if (truePositive + falseNegative) == 0 and (truePositive + falsePositive) ==0 :\n",
    "        return 0\n",
    "    elif truePositive + falseNegative == 0:\n",
    "        recal =0\n",
    "    elif truePositive + falsePositive ==0:\n",
    "        prec =0\n",
    "    else:\n",
    "        recal = truePositive / ( truePositive + falseNegative )  \n",
    "        prec = truePositive / (truePositive + falsePositive )  \n",
    "    \n",
    "    return ( (prec*recal) / (prec+recal) )*2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(Y, Y_predicted):\n",
    "    \n",
    "    matrix = np.zeros((2,2))\n",
    "    \n",
    "    truePositive= 0\n",
    "    trueNegative = 0 \n",
    "    falseNegative = 0\n",
    "    falsePositive = 0\n",
    "    #truePositive = np.sum(Y == Y_predicted) #.astype(np.int)\n",
    "    \n",
    "    for i, k in zip(Y, Y_predicted):\n",
    "        if i == 0 and k == 0 :  \n",
    "            trueNegative +=1\n",
    "        if i == 1 and k == 0 :\n",
    "            falseNegative +=1\n",
    "        if i == 0 and k == 1 :\n",
    "            falsePositive += 1\n",
    "        if i == 1 and k ==1:\n",
    "            truePositive += 1\n",
    "     \n",
    "    matrix[0][0] = trueNegative\n",
    "    matrix[0][1] = falsePositive\n",
    "    matrix[1][1] = truePositive\n",
    "    matrix[1][0] = falseNegative\n",
    "            \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Write a function to generate the Receiver Operating Characteristic (ROC) curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds=  [ 1, 0.9,   0.73,  0.33,  0.2, 0.1, 0.06 ]\n",
    "\n",
    "\n",
    "def roc_curve(thresholds, y_train, predict_prob, confus_matrix, recall):\n",
    "    \n",
    "    classified = np.zeros(len(predict_prob))\n",
    "    fpr = [] \n",
    "    tpr = []\n",
    "    for i, t in enumerate(thresholds):\n",
    "        for j, p in enumerate(predict_prob):\n",
    "            if p > t:\n",
    "                classified[j] = 1\n",
    "            else:\n",
    "                classified[j] = 0\n",
    "        \n",
    "        tpr.append( recall(y_train,classified) )\n",
    "        \n",
    "        conf_table =   confus_matrix(y_train, classified)\n",
    "        fpr.append( conf_table[0][1] / (conf_table[0][1] + conf_table[0][0]) ) # FP/FP+TN\n",
    "    return fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc_curve(fpr, tpr, label =None):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # these 3 line below are unnecessaary\n",
    "#     conf_matrix = confusion_matrix(Y, Y_predicted)\n",
    "#     FPR = conf_matrix[0][1]/ (conf_matrix[0][1] + conf_matrix[0][0])\n",
    "#     TPR = recall(Y, Y_predicted)\n",
    "    \n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(fpr, tpr, color='darkorange', linewidth=8, label=label) \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.title('ROC Curve (Test Data)')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Write a function to generate the Receiver Operating Characteristic (ROC) curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# import operator\n",
    "\n",
    "def auc_from_fpr_tpr(y_axis, x_axis_spacing):#(fpr, tpr, trapezoid=False):\n",
    "#     inds = [i for (i, (s, e)) in enumerate(zip(fpr[: -1], fpr[1: ])) if s != e] + [len(fpr) - 1]\n",
    "#     fpr, tpr = fpr[inds], tpr[inds]\n",
    "#     area = 0\n",
    "#     ft = zip(fpr, tpr)\n",
    "#     for p0, p1 in zip(ft[: -1], ft[1: ]):\n",
    "#         area += (p1[0] - p0[0]) * ((p1[1] + p0[1]) / 2 if trapezoid else p0[1])\n",
    "   \n",
    "    return  np.trapz(y_axis, dx = x_axis_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. KNN Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(object): \n",
    "    \n",
    "    def __init__(self):\n",
    "        #self.k = k\n",
    "        pass\n",
    "    \n",
    "    # distance_f is a function\n",
    "    def fit(self, training_features, training_labels, k, distance_f, **kwargs):\n",
    "        \n",
    "        self.training_features = training_features\n",
    "        self.training_labels = training_labels\n",
    "        self.k = k\n",
    "        self.distance_f = distance_f\n",
    "        self.distance = 0\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    \n",
    "    # predict\n",
    "    # Need to import:\n",
    "    # from sortedcontainers import SortedList\n",
    "    # import numpy as np\n",
    "    \n",
    "    distance = 0\n",
    "    def predict(self, test_features):\n",
    "        \n",
    "        test_features = np.array(test_features)\n",
    "        \n",
    "        y = np.zeros(len(test_features))\n",
    "        predict_prob = np.zeros(len(test_features))\n",
    "        \n",
    "        #scores = []\n",
    "        \n",
    "        for i, x in enumerate(test_features):\n",
    "            # to store (distance, label) turples\n",
    "            sl = SortedList() \n",
    "           \n",
    "            for j, xt in enumerate(self.training_features):\n",
    "                \n",
    "                distance = self.distance_f(x, xt)\n",
    "                \n",
    "                if len(sl) < self.k:\n",
    "                   \n",
    "                    sl.add(  (distance, self.training_labels[j])   )\n",
    "                else:\n",
    "                    if distance < sl[-1][0]:\n",
    "                        del sl[-1]\n",
    "                        sl.add( ( distance, self.training_labels[j] )  )      \n",
    "            \n",
    "            # count how many time a label appears in the sorted list\n",
    "            labelCount = {}\n",
    "            for _, l in sl:\n",
    "                labelCount[l] = labelCount.get(l, 0) + 1 # get() return 0 if label not found; the value otherwise\n",
    "            \n",
    "            # Classify; find the label that appears the most\n",
    "            max_votes = 0\n",
    "            label = -1\n",
    "            for l, labelCount in iteritems(labelCount):\n",
    "                if labelCount > max_votes:\n",
    "                    max_votes = labelCount\n",
    "                    label = l\n",
    "            \n",
    "            y[i] = label\n",
    "            \n",
    "            # This will help determine the prediction probability for each data points\n",
    "            predict_prob[i] = max_votes/k\n",
    "            \n",
    "            #scores.append((label, predict_prob))\n",
    "            \n",
    "        return y,predict_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Read in the file as a pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'datasets/winequality-white.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0318ae878103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/winequality-white.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'datasets/winequality-white.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"datasets/winequality-white.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Convert target into a two-category variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"quality\"] = (df[\"quality\"] > 5).astype(np.int)\n",
    "df.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Summary of each variable in terms of mean, standard deviation, and quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17 Shuffle the rows without affecting the order of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  df.sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Generate pair plot using seaborn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "def corrfunc(x, y, **kws):\n",
    "    r, _ = stats.pearsonr(x, y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.1, .6), xycoords=ax.transAxes,\n",
    "               size = 24)\n",
    "    \n",
    "cmap = sns.cubehelix_palette(light=1, dark = 0.1,\n",
    "                             hue = 0.5, as_cmap=True)\n",
    "\n",
    "sns.set_context(font_scale=2)\n",
    "\n",
    "# Pair grid set up\n",
    "g = sns.PairGrid(df)\n",
    "\n",
    "# Scatter plot on the upper triangle\n",
    "g.map_upper(plt.scatter, s=10, color = 'red')\n",
    "\n",
    "# Distribution on the diagonal\n",
    "g.map_diag(sns.distplot, kde=False, color = 'red')\n",
    "\n",
    "# Density Plot and Correlation coefficients on the lower triangle\n",
    "g.map_lower(sns.kdeplot, cmap = cmap)\n",
    "g.map_lower(corrfunc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Drop the redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for numerical correlations between the attributes and target\n",
    "df.corr().abs()['quality'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation: density vs others\n",
    "df.corr().abs()['density'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because 'residual sugar' and 'alcohol' are highly correlated to density, let check there are related to each other\n",
    "df.corr().abs()['alcohol'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because 'residual sugar' and 'alcohol' are not highly correlated, we drop only 'density'\n",
    "df = df.drop(['density' ], 1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Function to partition the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes 3 arguments: feature matrix (numpy array with rows representing data samples and columns representing \n",
    "features.), target vector (numpy array with labels corresponding to each row of the feature matrix), and t ( a real number\n",
    "to determine the size of partition). \n",
    "\"\"\"\n",
    "def partition(features, target, t):\n",
    "#     # Check if 'features' or 'target' is an instance of 'np.ndarray'\n",
    "#     features = np.zeros(len(np_features))\n",
    "#     target = np.zeros(len(np_features))\n",
    "#     if isinstance(np_features, np.ndarray):\n",
    "#         features = pd.DataFrame(np_features, index=range(np_features.shape[0]),\n",
    "#                           columns=range(np_features.shape[1]))\n",
    "#     if isinstance(np_target, np.ndarray):\n",
    "#         target = pd.DataFrame(np_target, index=range(np_target.shape[0]),\n",
    "#                           columns=range(np_target.shape[1]))\n",
    "        \n",
    "        \n",
    "#     rowIndex = int((features.shape[0]*t))\n",
    "    \n",
    "#     X_test = np.array( features.iloc[:rowIndex]   )\n",
    "#     X_train = np.array( features.iloc[rowIndex:] )\n",
    "\n",
    "#     target_test = np.array( target.iloc[:rowIndex]  )\n",
    "#     target_train = np.array( target.iloc[rowIndex:] )\n",
    "\n",
    "    # create f and t; type np\n",
    "    feat = np.zeros(target.shape[0])\n",
    "    tar = np.zeros(target.shape[0])\n",
    "    #cast DataFrame to np.array\n",
    "    feat = np.array(features)\n",
    "    tar = np.array(target)\n",
    "    \n",
    "    index = int(target.shape[0]*t)\n",
    "\n",
    "    X_test = feat[:index,:]\n",
    "    X_train = feat[index:, :]\n",
    "    \n",
    "    y_test = tar[:index]\n",
    "    y_train = tar[index:]\n",
    "    \n",
    "    return  X_train, X_test, y_train, y_test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Naively run your kNN model on the train dataset with k = 5 and using Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = partition(df.drop([\"quality\"], axis=1), df[\"quality\"], t =0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sortedcontainers import SortedList\n",
    "from future.utils import iteritems\n",
    "\n",
    "k= 5\n",
    "knn = KNN()\n",
    "knn.fit(X_train, y_train, k, euclidianDistance)\n",
    "\n",
    "y_predicted, predict_prob = knn.predict(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check that the roc_curve and plot_roc_curve methods work\n",
    "\n",
    "thresholds=  [ 1, 0.8,  0.6, 0.4, 0.2 ]\n",
    "\n",
    "fpr, tpr= roc_curve(thresholds, y_train, predic_prob, confusion_matrix, recall)\n",
    "plot_roc_curve(fpr, tpr, label =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area under AUC\n",
    "auc_from_fpr_tpr( tpr, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Use accuracy and F1 score to compare your predictions to the expected label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a matrix: accurary and generalizationerror\n",
    "accuracy = accuracyGeneralizationError(y_train, y_predicted)\n",
    "accuracy[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = F1_score(y_train, y_predicted)\n",
    "F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Standardize each feature of your training set (subtract mean and divide by standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(X_train, X_test):\n",
    "    \n",
    "    mean = np.zeros(X_train.shape[1])\n",
    "    sdt = np.zeros(X_train.shape[1])\n",
    "    \n",
    "    for index in range( 0, X_train.shape[1] ):\n",
    "        mean[index] = X_train[:,index].mean()\n",
    "        sdt[index] = X_train[:, index].std()\n",
    "        X_train[:, index] =  ( X_train[:, index] - mean[index]  ) / sdt[index]\n",
    "        #print( dataset[:, index])\n",
    "    for i in range ( 0, X_test.shape[1] ):\n",
    "        X_test[:, index] =  ( X_test[:, index] - mean[i]  ) / sdt[i]\n",
    "        \n",
    "    return X_train, X_test\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stand, X_test_stand = standardization(X_train, X_test)\n",
    "#X_test_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Rerun the kNN model on the standardized data, find the accuracy and F1 score with the expected labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn = KNN()\n",
    "knn.fit(X_train_stand, y_train, 5, euclidianDistance)\n",
    "'''????????'''\n",
    "y_predicted, predic_prob = knn.predict(X_train_stand) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = accuracyGeneralizationError(y_train, y_predicted)\n",
    "print(\"Accuracy: \", accuracy[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = F1_score(y_train, y_predicted)\n",
    "print(\"F1 score: \", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a better score after scaling the data. Thus we will use standardized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. Write the S-fold (aka k-Fold) cross-validation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S-partition of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xtrain = X_train\n",
    "ytrain = y_train\n",
    "\n",
    "\n",
    "folds = 5\n",
    "folds_size = int(X_train.shape[0]/5)\n",
    "i=0\n",
    "start=0\n",
    "end= folds_size\n",
    "# print(X_train[start:end])\n",
    "train_data = []\n",
    "while i < folds:\n",
    "\n",
    "    train_data.append( (start, end))\n",
    "    start= end+1\n",
    "    end += folds_size\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "int(X_train.shape[0]/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_fold(folds,xtrain, ytrain, KNN, error_function, **kwargs):\n",
    "    \n",
    "    F1 = 0\n",
    "    \n",
    "    expected_predicted_labels = []\n",
    "#     folds = 5\n",
    "    folds_size = int(X_train.shape[0]/5)\n",
    "    \n",
    "    def S_partition():\n",
    "    \n",
    "        i=0\n",
    "        start=0\n",
    "        end= folds_size\n",
    "        # print(X_train[start:end])\n",
    "        train_data_index = []\n",
    "        \n",
    "        while i < folds:\n",
    "            train_data_index.append( (start, end))\n",
    "            start= end+1\n",
    "            end += folds_size\n",
    "            i+=1\n",
    "            \n",
    "        return train_data_index\n",
    "        \n",
    "    index = S_partition()\n",
    "    \n",
    "    for i,j in enumerate(index):\n",
    "           \n",
    "        k =kwargs[\"k\"]\n",
    "        fct = kwargs[\"distance\"]\n",
    "        knn = KNN()\n",
    "        knn.fit( np.delete(X_train, np.s_[j[0]: j[1]], axis=0), np.delete(y_train, np.s_[j[0]: j[1]], axis=0), k, fct )\n",
    "        '''????????????????????????????'''\n",
    "        y_predicted, predic_prob = knn.predict( y_train[ j[0]: j[1] ] )\n",
    "        \n",
    "#         print(\"y_train[ j[0]: j[1] ]\", y_train[ j[0]: j[1] ])\n",
    "        \n",
    "        F1 += error_function( np.delete(y_train, np.s_[j[0]: j[1]], axis=0), y_predicted )\n",
    "                \n",
    "        expected_predicted_labels.append( (np.delete(y_train, np.s_[j[0]: j[1]], axis=0), y_predicted) )\n",
    "      \n",
    "    # return expected labels, predicted labels , average error (F1)   \n",
    "    return expected_predicted_labels, F1/folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  23. Use your S-fold function to evaluate the performance of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np_df[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [1, 5, 9, 11]\n",
    "distance =  [ \"Euclidian\", \"Manhattan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = { }\n",
    "result = {}\n",
    "result_s = []\n",
    "for folds in K:\n",
    "    dic[\"k\"] = folds \n",
    "    for j, d in enumerate(distance):\n",
    "        if d == \"Euclidian\":\n",
    "            dic[\"distance\"] = euclidianDistance\n",
    "        else:\n",
    "            dic[\"distance\"] = ManhattanDistance\n",
    "         \n",
    "        exp_pred_labels , f1 = s_fold(10,X_train, y_train, KNN, F1_score, **dic)\n",
    "        result[\"k\"] = folds\n",
    "        result[\"distance\"] = d\n",
    "        result[\"F1\"] = f1\n",
    "        result[\"expected_labels\"] = exp_pred_labels[0]\n",
    "        result[\"predicted_labels\"] = exp_pred_labels[1]\n",
    "        \n",
    "        \n",
    "        result_s.append(result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. Evaluate your model on the test data and report the performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_predicted, tes_predic_prob = knn.predict(y_test)\n",
    "test_y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_precision = precision(y_test, y_predicted)\n",
    "print(\"precision: \",test_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recall = recall(y_test, y_predicted)\n",
    "print(\"recall: \",test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_F1 = F1_score(y_test, y_predicted)\n",
    "print(\"F1 score: \", test_F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = confusion_matrix(y_test, y_predicted)\n",
    "print(\"confusion matrix: \",test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracyGeneralizationError(y_train, y_predicted)\n",
    "print(\"Accuracy: \", accuracy[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds=  [ 1, 0.9,   0.73,  0.33,  0.2, 0.1, 0.06 ]\n",
    "\n",
    "fpr, tpr= roc_curve(thresholds, y_train, tes_predic_prob, confusion_matrix, recall)\n",
    "plot_roc_curve(fpr, tpr, label =None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. Calculate and report the 95% confidence interval on the generalization error estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# z = 1.96 for 95% confidence interval\n",
    "z = 1.96\n",
    "generalization_error = accuracy = accuracyGeneralizationError(y_train, y_predicted)[1]\n",
    "# standard_error = sqrt(p(1-p)/n)\n",
    "standard_error = math.sqrt(generalization_error*( 1- generalization_error) / y_train.shape[0])\n",
    "standard_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
